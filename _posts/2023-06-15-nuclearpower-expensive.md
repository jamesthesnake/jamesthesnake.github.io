Nuclear plant construction is often characterized as exhibiting “negative learning.” That is, instead of getting better at building plants over time, we’re getting worse. Plants have gotten radically more expensive, even as technology has improved and we understand the underlying science better. 


Nuclear power currently makes up slightly less than 20% of the total electricity produced in the U.S., largely from plants built in the 1970s and 80s. People are often enthusiastic about nuclear power because of its potential to decarbonize electricity production, produce electricity extremely cheaply and reduce the risk of grid disruption from weather events. 

But U.S. nuclear power has been hampered by steady and dramatic increases in nuclear power plant construction costs, frequently over the life of a single project. In the 1980s, several nuclear power plants in Washington were canceled after estimated construction costs increased from $4.1 billion to over $24 billion, resulting in a $2 billion bond default from the utility provider. Two reactors being built in Georgia (the only current nuclear reactors under construction in the U.S.) are projected to cost twice their initial estimates, and two South Carolina reactors were canceled after costs rose from $9.8 billion to $25 billion. Why are nuclear construction costs so high, and why do they so frequently increase? Let’s take a look.

Nuclear power plants cost more and more
The story of nuclear power plants in the U.S. is one of steadily rising costs to build them. Commercial plants whose construction began in the late 1960s cost $1000/KWe or less (in 2010 dollars); plants started just 10 years later cost nine times that much. Today the Vogtle 3 and 4 reactors are likely to come in at around $8000/KWe in overnight costs ($6000/KWe in 2010 dollars), with an actual cost of nearly double that due to financing costs.

