We all have seen this image from the Attention is All We Need paper. Looks scary, right? Let’s try to understand how this actually works using one single example and try to make this journey as simple as possible!
## Part 1 (Preprocessing)

### Dataset

First we must have a dataset, with which we will be working throughout our journey. For example, the dataset used in GPT3 is 570GB! We can’t obviously use that here as an example, so let’s make a short dataset with only 3 sentences
## Conclusion

This was a brief and short guide into how transformers work. Hope you guys enjoyed it :)

Sources :

1. Attention is all you need - Paper
2. Umar Jamil - Youtube
3. 3Blue1Brown - Youtube
4. Lots of Blogs from Medium
5. Me, for doing all the calculations by hand :’)
